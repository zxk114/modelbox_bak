name: Publish cuda112-develop-ubuntu
on:
  workflow_dispatch:
  push:
    branches:
      - 'main'
    tags:
      - v*
env:
  BUILD_TYPE: Release
  CUDA_VER: "11-2"
  CUDA_VERSION: "11.2"
  CUDA_CUDART_VERSION: "11.2.152-1"
  NVIDIA_CUDA_VERSION: "11.2.2"
  NVIDIA_REQUIRE_CUDA: "'cuda>=11.2 brand=tesla,driver>=418,driver<419 brand=tesla,driver>=440,driver<441 brand=tesla,driver>=450,driver<451 brand=tesla,driver>=460,driver<461'"
  IMAGE_NAME_DEV: "modelbox/modelbox-develop-tensorflow_2.6.0-cuda_11.2-ubuntu-x86_64"
  IMAGE_NAME_RUN: "modelbox/modelbox-runtime-tensorflow_2.6.0-cuda_11.2-ubuntu-x86_64"
  IMAGE_VERSION: "v1.1.3"

jobs:
  compile:
    runs-on: ubuntu-latest
    container: ubuntu:18.04
    steps:
    - uses: actions/checkout@v2
    - name: Install dependencies
      run: |
        ln -snf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime
        echo "Asia/Shanghai" > /etc/timezone
        apt-get update
        apt-get upgrade -y
        apt-get remove --purge python3-apt
        apt-get install --no-install-recommends -y dbus iproute2 gnupg2 curl ca-certificates
        apt-get install -y python3.7-dev python3-pip python3-apt python3-setuptools
        rm -f /usr/bin/python3 /usr/bin/python
        ln -s python3.7 /usr/bin/python3
        ln -s python3 /usr/bin/python
        apt-get install -y \
            build-essential unzip ffmpeg sudo bash vim gdb git doxygen autoconf cmake openssh-server \
            python3-wheel python3-numpy python3-opencv libopencv-dev pkg-config kmod net-tools pciutils \
            libssl-dev libcpprest-dev libswscale-dev libavformat-dev graphviz libgraphviz-dev libfuse-dev \
            libprotobuf-c-dev protobuf-c-compiler duktape-dev libmosquitto-dev
    - name: Install cuda
      run: |
        curl -fsSL https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub | apt-key add -
        cp -af ./docker/repo/*.list /etc/apt/sources.list.d/
        apt-get update
        apt install -y --no-install-recommends \
            libcudnn8=8.1.1.33-1+cuda11.2 \
            libcudnn8-dev=8.1.1.33-1+cuda11.2 \
            libcublas=11-2-11.4.1.1043-1 \
            libcublas-dev=11-2-11.4.1.1043-1
        apt install -y --no-install-recommends \
            cuda-cudart-${CUDA_VER}=${CUDA_CUDART_VERSION} \
            cuda-compat-${CUDA_VER} \
            cuda-minimal-build-${CUDA_VER} \
            cuda-libraries-dev-${CUDA_VER} \
            cuda-command-line-tools-${CUDA_VER}
        ln -s cuda-${CUDA_VERSION} /usr/local/cuda
        rm -rf /var/lib/apt/lists/*
    - name: Install ThirdParty
      run: |
        curl https://nodejs.org/dist/v16.13.2/node-v16.13.2-linux-x64.tar.xz|tar -xJ
        cp -af node-v16.13.2-linux-x64/{bin,include,lib,share} /usr/local/
        npm install -g npm@latest
        npm install -g @angular/cli
        npm cache clean --force

        curl -LJO https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-gpu-linux-x86_64-2.6.0.tar.gz
        curl -LJO https://download.java.net/java/GA/jdk17/0d483333a00540d886896bac774ff48b/35/GPL/openjdk-17_linux-x64_bin.tar.gz
        curl -LJO https://dlcdn.apache.org/maven/maven-3/3.8.4/binaries/apache-maven-3.8.4-bin.tar.gz
        curl -LJO https://github.com/zxk114/modelbox/releases/download/binary/obs-dev.tar.gz
        for tar in *.tar.gz; do tar zxf $tar -C /usr/local/; done
        rm -f libtensorflow-gpu-linux-x86_64-2.6.0.tar.gz

        curl -LJO https://github.com/zxk114/modelbox/releases/download/binary/Video_Codec_SDK_9.1.23.zip
        unzip -j Video_Codec_SDK_9.1.23.zip \
            Video_Codec_SDK_9.1.23/include/cuviddec.h \
            Video_Codec_SDK_9.1.23/include/nvcuvid.h \
            Video_Codec_SDK_9.1.23/include/nvEncodeAPI.h \
            -d /usr/local/include
        unzip -j Video_Codec_SDK_9.1.23.zip \
            Video_Codec_SDK_9.1.23/Lib/linux/stubs/x86_64/libnvcuvid.so \
            Video_Codec_SDK_9.1.23/Lib/linux/stubs/x86_64/libnvidia-encode.so \
            -d /usr/local/lib
        ln -s libnvcuvid.so /usr/local/lib/libnvcuvid.so.1
        cp -af /usr/local/cuda/compat/* /usr/local/lib/

        yes | sudo python3 -m pip install --upgrade pip
        yes | sudo pip3 install pillow numpy wheel
    - name: CMake
      run: |
        pwd
        export JAVA_HOME=/usr/lib/jvm/jdk-17
        export MAVEN_HOME=/usr/local/apache-maven-3.8.3
        export LD_LIBRARY_PATH=/usr/local/cuda/lib64/stubs${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}
        export PATH=/usr/local/cuda/bin:$JAVA_HOME/bin:$MAVEN_HOME/bin${PATH:+:${PATH}}
        export PKG_CONFIG_PATH=/usr/lib/x86_64-linux-gnu/pkgconfig${PKG_CONFIG_PATH:+:${PKG_CONFIG_PATH}}
        ls -lh .
        ldconfig
        mkdir build
        cd build
        cmake .. -DCMAKE_BUILD_TYPE=${{env.BUILD_TYPE}} -DWITH_WEB_UI=on
    - name: Build
      working-directory: ${{github.workspace}}/build
      run: |
        make package -j4
        rm -f release/*.rpm
        ls -lh release
        filecount=$(ls release | wc -l)
        dpkgcount=$(ls release | egrep "*.deb" | wc -l)
        artifacts_file=$(ls ${artifacts_path} | grep "cuda"| wc -l)
        if [ ${filecount} -ge 13 ] && [ ${dpkgcount} -ge 11 ] && [ ${artifacts_file} -eq 2 ]; then
            echo "compile success"
        else
            echo "compile failed"
            exit 1
        fi
    - name: prepare-artifact
      run: |
        pwd
        mkdir ./artifact
        ls -lh .
        cp -af ./*.tar.gz ./artifact/
        cp -af ./build/release ./artifact/
        ls -lh ./artifact
    - name: upload-artifact
      uses: actions/upload-artifact@v2
      with:
        name: modelbox-artifact
        path: ./artifact

  build_develop_image:
    runs-on: ubuntu-latest
    needs: compile
    steps:
      - name: Checkout
        uses: actions/checkout@v2
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v1
      - name: Login to DockerHub
        uses: docker/login-action@v1
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}
      - name: download-artifact
        uses: actions/download-artifact@v2
        with:
          name: modelbox-artifact
          path: ${{github.workspace}}/
      - name: Prepare
        run: |
          ls -lh ${{github.workspace}}/*
      - name: Build and push
        id: docker_build
        uses: docker/build-push-action@v2
        with:
          push: false
          context: .
          file: ./docker/Dockerfile.cuda.develop
          build-args: |
            CUDA_VER=${{ env.CUDA_VER }}
            CUDA_VERSION=${{ env.CUDA_VERSION }}
            CUDA_CUDART_VERSION=${{ env.CUDA_CUDART_VERSION }}
            NVIDIA_CUDA_VERSION=${{ env.NVIDIA_CUDA_VERSION }}
            NVIDIA_REQUIRE_CUDA=${{ env.NVIDIA_REQUIRE_CUDA }}
          tags: |
            ${{ env.IMAGE_NAME_DEV }}:latest
            ${{ env.IMAGE_NAME_DEV }}:${{ env.IMAGE_VERSION }}

  build_runtime_image:
    runs-on: ubuntu-latest
    needs: compile
    steps:
      - name: Checkout
        uses: actions/checkout@v2
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v1
      - name: Login to DockerHub
        uses: docker/login-action@v1
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}
      - name: download-artifact
        uses: actions/download-artifact@v2
        with:
          name: modelbox-artifact
          path: ${{github.workspace}}/
      - name: Prepare
        run: |
          ls -lh ${{github.workspace}}/*
      - name: Build and push
        id: docker_build
        uses: docker/build-push-action@v2
        with:
          push: false
          context: .
          file: ./docker/Dockerfile.cuda.runtime
          build-args: |
            CUDA_VER=${{ env.CUDA_VER }}
            CUDA_VERSION=${{ env.CUDA_VERSION }}
            CUDA_CUDART_VERSION=${{ env.CUDA_CUDART_VERSION }}
            NVIDIA_CUDA_VERSION=${{ env.NVIDIA_CUDA_VERSION }}
            NVIDIA_REQUIRE_CUDA=${{ env.NVIDIA_REQUIRE_CUDA }}
          tags: |
            ${{ env.IMAGE_NAME_RUN }}:latest
            ${{ env.IMAGE_NAME_RUN }}:${{ env.IMAGE_VERSION }}
