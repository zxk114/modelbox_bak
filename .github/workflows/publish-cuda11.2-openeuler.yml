name: Publish cuda112-tensorflow-openeuler
on:
  workflow_dispatch:
  push:
    tags:
      - 'v*'
env:
  BUILD_TYPE: Release
  CUDA_VER: "11-2"
  CUDA_VERSION: "11.2"
  CUDA_CUDART_VERSION: "11.2.152-1"
  NVIDIA_CUDA_VERSION: "11.2.2"
  NVIDIA_REQUIRE_CUDA: "'cuda>=11.2 brand=tesla,driver>=418,driver<419 brand=tesla,driver>=440,driver<441 brand=tesla,driver>=450,driver<451 brand=tesla,driver>=460,driver<461'"
  IMAGE_NAME_DEV: "zxk114/modelbox-develop-tensorflow_2.6.0-cuda_11.2-openeuler-x86_64"
  IMAGE_NAME_RUN: "zxk114/modelbox-runtime-tensorflow_2.6.0-cuda_11.2-openeuler-x86_64"
  IMAGE_VERSION: "v1.1.3"

jobs:
  compile:
    runs-on: ubuntu-latest
    container: openeuler/openeuler:20.03-lts-sp2
    steps:
    - uses: actions/checkout@v2
    - name: Install dependencies
      run: |
        ln -snf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime
        echo "Asia/Shanghai" > /etc/timezone
        yum update -y && \
        yum install -y vim gcc gcc-c++ make cmake libnsl python3-perf rsyslog doxygen pciutils gdb net-tools zlib-devel \
                rpm-build gcc-gfortran alsa-lib nss-devel fuse-devel gdbm-devel krb5-devel expat-devel curl-devel bzip2-devel \
                boost-devel ncurses-devel libxml2-devel libssh2-devel openssh-server python3-devel glibc-debuginfo libatomic \
                git clang xauth graphviz-devel sqlite-devel openssl-devel readline-devel mesa-libGL-devel protobuf-c-devel && \
        ls -lh /usr/bin/python* && ln -s python3 /usr/bin/python && yum clean all && \
        rm -rf /var/cache /tmp/* /var/tmp/*
    - name: Install cuda
      run: |
        NVIDIA_GPGKEY_SUM=d1be581509378368edeec8c1eb2958702feedf3bc3d17011adbf24efacce4ab5 && \
        curl -fsSL https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/7fa2af80.pub | sed '/^Version/d' > /etc/pki/rpm-gpg/RPM-GPG-KEY-NVIDIA && \
        echo "$NVIDIA_GPGKEY_SUM  /etc/pki/rpm-gpg/RPM-GPG-KEY-NVIDIA" | sha256sum -c --strict -
        cp -af ./docker/repo/cuda.repo /etc/yum.repos.d/
        yum install -y --nogpgcheck --setopt=obsoletes=0 \
            cuda-cudart-${CUDA_VER}-${CUDA_CUDART_VERSION} \
            cuda-compat-${CUDA_VER} \
            cuda-minimal-build-${CUDA_VER} \
            cuda-libraries-devel-${CUDA_VER} \
            cuda-command-line-tools-${CUDA_VER} \
            libcudnn8-8.1.1.33-1.cuda11.2 \
            libcudnn8-devel-8.1.1.33-1.cuda11.2 \
            libcublas-11-2-11.4.1.1043-1 \
            libcublas-devel-11-2-11.4.1.1043-1
        ln -s cuda-${CUDA_VERSION} /usr/local/cuda
        cp -af /usr/local/cuda/compat/* /usr/local/lib64/
    - name: Install ThirdParty
      run: |
        curl https://nodejs.org/dist/v16.13.2/node-v16.13.2-linux-x64.tar.xz|tar -xJ
        cp -af node-v16.13.2-linux-x64/{bin,include,lib,share} /usr/local/
        npm install -g npm@latest
        npm install -g @angular/cli
        npm cache clean --force

        curl -LJO https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-gpu-linux-x86_64-2.6.0.tar.gz
        curl -LJO https://download.java.net/java/GA/jdk17/0d483333a00540d886896bac774ff48b/35/GPL/openjdk-17_linux-x64_bin.tar.gz
        curl -LJO https://dlcdn.apache.org/maven/maven-3/3.8.4/binaries/apache-maven-3.8.4-bin.tar.gz
        curl -LJO https://github.com/zxk114/modelbox/releases/download/binary/obs-dev.tar.gz
        curl -LJO https://github.com/zxk114/modelbox/releases/download/binary/cpprestsdk-dev.tar.gz
        curl -LJO https://github.com/zxk114/modelbox/releases/download/binary/duktape-dev.tar.gz
        curl -LJO https://github.com/zxk114/modelbox/releases/download/binary/ffmpeg-dev.tar.gz
        curl -LJO https://github.com/zxk114/modelbox/releases/download/binary/opencv-dev.tar.gz
        for tar in *.tar.gz; do tar zxf $tar -C /usr/local/; done

        curl -LJO https://github.com/zxk114/modelbox/releases/download/binary/Video_Codec_SDK_9.1.23.zip
        unzip -j Video_Codec_SDK_9.1.23.zip \
            Video_Codec_SDK_9.1.23/include/cuviddec.h \
            Video_Codec_SDK_9.1.23/include/nvcuvid.h \
            Video_Codec_SDK_9.1.23/include/nvEncodeAPI.h \
            -d /usr/local/include
        unzip -j Video_Codec_SDK_9.1.23.zip \
            Video_Codec_SDK_9.1.23/Lib/linux/stubs/x86_64/libnvcuvid.so \
            Video_Codec_SDK_9.1.23/Lib/linux/stubs/x86_64/libnvidia-encode.so \
            -d /usr/local/lib64
        ln -s libnvcuvid.so /usr/local/lib64/libnvcuvid.so.1

        mkdir -p /root/.pip
        echo "[global]" > /root/.pip/pip.conf
        echo "index-url = https://pypi.mirrors.ustc.edu.cn/simple" >>/root/.pip/pip.conf
        echo "trusted-host = pypi.mirrors.ustc.edu.cn" >>/root/.pip/pip.conf
        echo "timeout = 120" >>/root/.pip/pip.conf 
        yes | python3 -m pip install --upgrade pip
        yes | pip3 install pillow numpy wheel
    - name: CMake
      run: |
        pwd
        export JAVA_HOME=/usr/lib/jvm/jdk-17
        export MAVEN_HOME=/usr/local/apache-maven-3.8.3
        export LD_LIBRARY_PATH=/usr/local/cuda/lib64/stubs${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}
        export PATH=/usr/local/cuda/bin:$JAVA_HOME/bin:$MAVEN_HOME/bin${PATH:+:${PATH}}
        export PKG_CONFIG_PATH=/usr/lib/x86_64-linux-gnu/pkgconfig${PKG_CONFIG_PATH:+:${PKG_CONFIG_PATH}}
        ls -lh .
        ldconfig
        mkdir build
        cd build
        cmake .. -DCMAKE_BUILD_TYPE=${{env.BUILD_TYPE}} -DWITH_WEB_UI=on
    - name: Build
      working-directory: ${{github.workspace}}/build
      run: |
        make package -j4
        ls -lh release
        filecount=$(ls release | wc -l)
        dpkgcount=$(ls release | egrep "*.rpm" | wc -l)
        artifacts_file=$(ls ${artifacts_path} | grep "cuda"| wc -l)
        if [ ${filecount} -ge 13 ] && [ ${dpkgcount} -ge 11 ] && [ ${artifacts_file} -eq 2 ]; then
            echo "compile success"
        else
            echo "compile failed"
            exit 1
        fi
    - name: prepare-artifact
      run: |
        pwd
        mkdir ./artifact
        ls -lh .
        cp -af ./*.tar.gz ./artifact/
        cp -af ./build/release ./artifact/
        ls -lh ./artifact
    - name: upload-artifact
      uses: actions/upload-artifact@v2
      with:
        name: modelbox-artifact
        path: ./artifact
  
  build_develop_image:
    runs-on: ubuntu-latest
    needs: compile
    steps:
      - name: Checkout
        uses: actions/checkout@v2
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v1
      - name: Login to DockerHub
        uses: docker/login-action@v1
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME_zxk }}
          password: ${{ secrets.DOCKERHUB_TOKEN_zxk }}
      - name: download-artifact
        uses: actions/download-artifact@v2
        with:
          name: modelbox-artifact
          path: ${{github.workspace}}/
      - name: Prepare
        run: |
          ls -lh ${{github.workspace}}/*
      - name: Build and push
        id: docker_build
        uses: docker/build-push-action@v2
        with:
          push: false
          context: ${{github.workspace}}
          file: ./docker/Dockerfile.cuda.develop.openeuler
          build-args: |
            CUDA_VER=${{ env.CUDA_VER }}
            CUDA_VERSION=${{ env.CUDA_VERSION }}
            CUDA_CUDART_VERSION=${{ env.CUDA_CUDART_VERSION }}
            NVIDIA_CUDA_VERSION=${{ env.NVIDIA_CUDA_VERSION }}
            NVIDIA_REQUIRE_CUDA=${{ env.NVIDIA_REQUIRE_CUDA }}
          tags: |
            ${{ env.IMAGE_NAME_DEV }}:latest
            ${{ env.IMAGE_NAME_DEV }}:${{ env.IMAGE_VERSION }}

  build_runtime_image:
    runs-on: ubuntu-latest
    needs: compile
    steps:
      - name: Checkout
        uses: actions/checkout@v2
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v1
      - name: Login to DockerHub
        uses: docker/login-action@v1
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME_zxk }}
          password: ${{ secrets.DOCKERHUB_TOKEN_zxk }}
      - name: download-artifact
        uses: actions/download-artifact@v2
        with:
          name: modelbox-artifact
          path: ${{github.workspace}}/
      - name: Prepare
        run: |
          ls -lh ${{github.workspace}}/*
      - name: Build and push
        id: docker_build
        uses: docker/build-push-action@v2
        with:
          push: false
          context: ${{github.workspace}}
          file: ./docker/Dockerfile.cuda.runtime.openeuler
          build-args: |
            CUDA_VER=${{ env.CUDA_VER }}
            CUDA_VERSION=${{ env.CUDA_VERSION }}
            CUDA_CUDART_VERSION=${{ env.CUDA_CUDART_VERSION }}
            NVIDIA_CUDA_VERSION=${{ env.NVIDIA_CUDA_VERSION }}
            NVIDIA_REQUIRE_CUDA=${{ env.NVIDIA_REQUIRE_CUDA }}
          tags: |
            ${{ env.IMAGE_NAME_RUN }}:latest
            ${{ env.IMAGE_NAME_RUN }}:${{ env.IMAGE_VERSION }}
